{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Student ID: XXX\n","\n","**You student_id is your 7/8 digit faser number.**\n","\n","This is a sample format for CE807-24-SP: Assignment . You must follow the format.\n","The code will have three broad sections, and additional section, if needed,\n","\n","\n","1.   Common Codes\n","2.   Method/model 1 Specific Codes\n","3.   Method/model 2 Specific Codes\n","4.   Other Method/model Codes, if any\n","\n","**You must have `train_gen`, `test_gen` for Generative method  and `train_dis`, `test_dis` for Discriminatuve method to perform full training and testing. This will be evaluated automatically, without this your code will fail and no marked.**\n","\n","You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices.\n","\n","Before each `code block/function`, you must have a `text block` which explain what code block/function is going to do. For each function/class, you need to properly document what are it's input, functionality and output.\n","\n","If you are using any non-standard library, you must have command to install that, for example `pip install datasets`.\n","\n","You must print `train`, `validation` and `test` performance measures.\n","\n","You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.\n","\n","Your code must\n","\n","*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**.\n","*   read dataset from './student_id/data/number/', where number is last digit of your student_id folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]\n","*   save model after finishing the training in './student_id/Model_Gen/' and './student_id/Model_Dis/' for Generative and Discriminative model respectively.\n","*   at testing time you will load models from './student_id/Model_Gen/' and './student_id/Model_Dis/'  for Generative and Discriminative model respectively. Your output file based on the test file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.\n","*  after testing, your output file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.\n","\n","\n","\n","\n","**Install and import all required libraries first before starting to code.**\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"VgzEm1gDYBUp"}},{"cell_type":"markdown","source":["Let's install all require libraries. For example, `transformers`"],"metadata":{"id":"_3ZWJlO6JOqY"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"81A_4_dKJV4Y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fb82969-bdad-4393-b12a-e3d59e0220fb","executionInfo":{"status":"ok","timestamp":1713878035808,"user_tz":-60,"elapsed":8104,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"markdown","source":["Let's import all require libraries.\n","For example, `numpy`"],"metadata":{"id":"U5XEt6asIi3Q"}},{"cell_type":"code","source":["import numpy as np\n","import os\n","import pickle\n","import pandas as pd"],"metadata":{"id":"TKEZRYhIImbg","executionInfo":{"status":"ok","timestamp":1713878035808,"user_tz":-60,"elapsed":4,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["import re\n","import string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","import re\n","from gensim.parsing.preprocessing import remove_stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtCxWhIG2fv_","executionInfo":{"status":"ok","timestamp":1713878035808,"user_tz":-60,"elapsed":3,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}},"outputId":"4e5a5a14-d426-461b-de3f-30e12b03dbde"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["!pip install hmmlearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zKDMepBH6L2","executionInfo":{"status":"ok","timestamp":1713878047548,"user_tz":-60,"elapsed":11742,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}},"outputId":"cfa80f62-7c47-4fcd-c5b7-efd63f6e3010"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.25.2)\n","Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.4.0)\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","import torch.nn.functional as F\n","from sklearn.linear_model import LogisticRegression"],"metadata":{"id":"n1CZBhqpBdwM","executionInfo":{"status":"ok","timestamp":1713878047548,"user_tz":-60,"elapsed":7,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["**Let's put your student id as a variable, that you will use different places**"],"metadata":{"id":"pd5kSsAPZoE6"}},{"cell_type":"code","source":["student_id = 2310092 # Note this is an interger and you need to input your id"],"metadata":{"id":"rqP6pp_3ZkVy","executionInfo":{"status":"ok","timestamp":1713878047548,"user_tz":-60,"elapsed":5,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"],"metadata":{"id":"RiLUrQ-3zC6V"}},{"cell_type":"code","source":["# set same seeds for all libraries\n","\n","#numpy seed\n","np.random.seed(student_id)"],"metadata":{"id":"TYUn2tj3zCFq","executionInfo":{"status":"ok","timestamp":1713878047548,"user_tz":-60,"elapsed":4,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["# Common Codes\n","\n","In this section you will write all common codes, for examples\n","\n","\n","*   Data read\n","*   Command Line argument reading\n","*   Performance Matrics\n","*   Print Dataset Statistics\n","*   Saving model and output\n","*   Loading Model and output\n","*   etc\n","\n","\n"],"metadata":{"id":"Dlj_VQrkbLgM"}},{"cell_type":"markdown","source":["**Let's first allow the GDrive access and set data and model paths**\n","\n","For examples,\n","\n","student_id = 12345670\n","\n","set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807-24-SP/Lab10/’ in your GDrive\n","\n","now set all global variable,\n","\n","\n","Sample output directory and file structure: https://drive.google.com/drive/folders/1okgSzgGiwPYYFp7NScEt9MNVolOlld1d?usp=share_link   "],"metadata":{"id":"uOESFmIPn_nr"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"L1kvIe1NbDoS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9aaf8a34-f6ae-403e-bc0b-690c913c44c5","executionInfo":{"status":"ok","timestamp":1713878051180,"user_tz":-60,"elapsed":3636,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Add your code to initialize GDrive and data and models paths\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '/content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final'\n","GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))\n","\n","DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '2') # Make sure to replace 0 with last digit of your student Regitration number\n","train_file = os.path.join(DATA_PATH, 'train.csv')\n","print('Train file: ', train_file)\n","\n","val_file = os.path.join(DATA_PATH, 'valid.csv')\n","print('Validation file: ', val_file)\n","\n","test_file = os.path.join(DATA_PATH, 'test.csv')\n","print('Test file: ', test_file)\n","\n","\n","MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number\n","MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory\n","print('Model Generative directory: ', MODEL_Gen_DIRECTORY)\n","\n","MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'\n","\n","\n","MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory\n","print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)\n","\n","MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WKnbP3roLTj","outputId":"61092cf8-03a0-4c31-a05a-902bbdfaefcf","executionInfo":{"status":"ok","timestamp":1713878051181,"user_tz":-60,"elapsed":8,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["List files:  ['data', '2024-04-22 10-59-09.mkv', 'model', 'Toxic word prediction.pptx', 'text analysis final.ipynb']\n","Train file:  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/train.csv\n","Validation file:  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/valid.csv\n","Test file:  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/test.csv\n","Model Generative directory:  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/Model_Gen\n","Model Discriminative directory:  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/Model_Dis\n"]}]},{"cell_type":"markdown","source":["Let's see train file"],"metadata":{"id":"0pWvXDghtafa"}},{"cell_type":"code","source":["train_df = pd.read_csv(train_file)\n","train_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"DE20RqYGtZjZ","outputId":"e19661dc-c878-4ef8-aa74-b1fac9002c45","executionInfo":{"status":"ok","timestamp":1713878051543,"user_tz":-60,"elapsed":365,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    comment_id                                            comment  split  \\\n","0  578388428.0  SDATA_2 :  Apologies. This is the first time I...  train   \n","1  589196884.0  SDATA_2 :  `NEWLINE_TOKENNEWLINE_TOKEN== User ...  train   \n","2  651055158.0  SDATA_2 :   If you feel so please resubmit and...  train   \n","3  476344881.0  SDATA_2 :   IFA Premiership NEWLINE_TOKENNEWLI...  train   \n","4   83743858.0  SDATA_2 :  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_...  train   \n","\n","   toxicity  \n","0         0  \n","1         0  \n","2         0  \n","3         0  \n","4         1  "],"text/html":["\n","  <div id=\"df-030cdf5d-10e6-4153-866b-f77ed240199d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_id</th>\n","      <th>comment</th>\n","      <th>split</th>\n","      <th>toxicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>578388428.0</td>\n","      <td>SDATA_2 :  Apologies. This is the first time I...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>589196884.0</td>\n","      <td>SDATA_2 :  `NEWLINE_TOKENNEWLINE_TOKEN== User ...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>651055158.0</td>\n","      <td>SDATA_2 :   If you feel so please resubmit and...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>476344881.0</td>\n","      <td>SDATA_2 :   IFA Premiership NEWLINE_TOKENNEWLI...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>83743858.0</td>\n","      <td>SDATA_2 :  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_...</td>\n","      <td>train</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-030cdf5d-10e6-4153-866b-f77ed240199d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-030cdf5d-10e6-4153-866b-f77ed240199d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-030cdf5d-10e6-4153-866b-f77ed240199d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1e078dce-9874-460d-a167-8918db6004e5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e078dce-9874-460d-a167-8918db6004e5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1e078dce-9874-460d-a167-8918db6004e5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_df","summary":"{\n  \"name\": \"train_df\",\n  \"rows\": 8699,\n  \"fields\": [\n    {\n      \"column\": \"comment_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 200704603.48277646,\n        \"min\": 4216.0,\n        \"max\": 699826615.0,\n        \"num_unique_values\": 8699,\n        \"samples\": [\n          447988238.0,\n          59651578.0,\n          680890365.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8697,\n        \"samples\": [\n          \"SDATA_2 :  NEWLINE_TOKENNEWLINE_TOKEN== ( Unfortunately the South Park ended before Kramer ===NEWLINE_TOKENUnfortunately the South Park season ended before the whole Michael Richards controversy. This would have been pretty funny to see South Park make fun of Richards. Oh well! We can't put anything about this in the article because it didn't happen. South Park season was over so they couldn't make fun of the situation.    : EDATA_2\",\n          \"SDATA_2 :  NEWLINE_TOKENNEWLINE_TOKEN:The person who closed that discussion left open the possibility of reopening it, as a general discussion or article-by-article.  So, I might just do that, which is what the pro-diacritics crowd has done repeatedly.      : EDATA_2\",\n          \"SDATA_2 :  What annoys me is that stuff about us gets posted and we may not know about it. Can someone ping me next time this happens?   NEWLINE_TOKEN  : EDATA_2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxicity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["Let's show you a sample output file. Notice all fields, `out_label` is your model's output for that `tweet` and `id`"],"metadata":{"id":"Xnm3sUf-q0g4"}},{"cell_type":"markdown","source":["We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"],"metadata":{"id":"E-OjJ4REbhcj"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score"],"metadata":{"id":"eLOMgVbQfFSm","executionInfo":{"status":"ok","timestamp":1713878051544,"user_tz":-60,"elapsed":6,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def compute_performance(y_true, y_pred):\n","    \"\"\"\n","    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).\n","    This also display Confusion Matrix with proper X & Y axis labels.\n","    Also, returns F1 score\n","\n","    Args:\n","        y_true: numpy array or list\n","        y_pred: numpy array or list\n","\n","\n","    Returns:\n","        float\n","    \"\"\"\n","\n","    ##########################################################################\n","    #                     TODO: Implement this function                      #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","    score = f1_score(y_true, y_pred, average='macro')\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################\n","    return score"],"metadata":{"id":"aLuUu5BWcTid","executionInfo":{"status":"ok","timestamp":1713878051544,"user_tz":-60,"elapsed":6,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["\n","def save_model(model,model_dir):\n","  # save the model to disk\n","  # Check if the Model directory exists\n","\n","  # Note you might have to modify this based on your requirement\n","\n","  if not os.path.exists(model_dir):\n","      # Create the directory if it doesn't exist\n","      os.makedirs(model_dir)\n","      print(f\"Directory '{model_dir}' created successfully.\")\n","  else:\n","      print(f\"Directory '{model_dir}' already exists.\")\n","\n","  model_file = os.path.join(model_dir, 'model.sav')\n","  pickle.dump(model, open(model_file, 'wb'))\n","\n","  print('Saved model to ', model_file)\n","\n","  return model_file\n","\n","def load_model(model_file):\n","    # load model from disk\n","\n","    # Note you might have to modify this based on your requirement\n","\n","    model = pickle.load(open(model_file, 'rb'))\n","\n","    print('Loaded model from ', model_file)\n","\n","    return model"],"metadata":{"id":"hYwuqlPE5mnr","executionInfo":{"status":"ok","timestamp":1713878051544,"user_tz":-60,"elapsed":5,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["# Let's download GDrive Link into a directory"],"metadata":{"id":"Es_jkptAU4rK"}},{"cell_type":"code","source":["import requests\n","\n","def extract_file_id_from_url(url):\n","    # Extract the file ID from the URL\n","    file_id = None\n","    if 'drive.google.com' in url:\n","        file_id = url.split('/')[-2]\n","    elif 'https://docs.google.com' in url:\n","        file_id = url.split('/')[-1]\n","\n","    return file_id\n","\n","def download_file_from_drive(file_id, file_path):\n","    # Construct the download URL\n","    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n","\n","    # Download the file\n","    response = requests.get(download_url)\n","    if response.status_code == 200:\n","        with open(file_path, 'wb') as f:\n","            f.write(response.content)\n","        print(\"File downloaded successfully!\",file_path)\n","    else:\n","        print(\"Failed to download the file.\")\n","\n","def download_zip_file_from_link(file_url,file_path):\n","\n","  file_id = extract_file_id_from_url(file_url)\n","  if file_id:\n","      download_file_from_drive(file_id, file_path)\n","  else:\n","      print(\"Invalid Google Drive URL.\")\n"],"metadata":{"id":"crNmol_BUxbl","executionInfo":{"status":"ok","timestamp":1713878051544,"user_tz":-60,"elapsed":5,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["# Zip and Unzip a GDrive File"],"metadata":{"id":"PN9mRxr_VVzO"}},{"cell_type":"code","source":["import zipfile\n","import shutil\n","import os\n","\n","# Function to zip a directory\n","def zip_directory(directory, zip_filename):\n","    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, dirs, files in os.walk(directory):\n","            for file in files:\n","                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n","        print('Created a zip file',zip_filename)\n","\n","# Function to unzip a zip file\n","def unzip_file(zip_filename, extract_dir):\n","    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n","        zip_ref.extractall(extract_dir)\n","    print('Extracted a zip file to',extract_dir)\n","\n","# Example usage:\n","# directory_to_zip = 'path/to/your/directory'\n","# zip_filename = 'output_zipfile.zip'\n","\n","# # Zip the directory\n","# zip_directory(directory_to_zip, zip_filename)\n","\n","# # Unzip the zip file\n","# extract_dir = 'path/to/extract'\n","# unzip_file(zip_filename, extract_dir)\n"],"metadata":{"id":"8LIOzFFbVVL6","executionInfo":{"status":"ok","timestamp":1713878051544,"user_tz":-60,"elapsed":5,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["# Get Sharable link of your Zip file in Gdrive"],"metadata":{"id":"YrBaRZeDU9yE"}},{"cell_type":"code","source":["!pip install -U -q PyDrive"],"metadata":{"id":"VDPTDAGQglw-","executionInfo":{"status":"ok","timestamp":1713878064828,"user_tz":-60,"elapsed":13289,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","def get_gdrive_link(file_path):\n","    # Authenticate and create PyDrive client\n","    auth.authenticate_user()\n","    gauth = GoogleAuth()\n","    gauth.credentials = GoogleCredentials.get_application_default()\n","    drive = GoogleDrive(gauth)\n","\n","    # Find the file in Google Drive\n","    file_name = file_path.split('/')[-1]\n","    file_list = drive.ListFile({'q': f\"title='{file_name}'\"}).GetList()\n","\n","    # Get the file ID and generate the shareable link\n","    if file_list:\n","        file_id = file_list[0]['id']\n","        gdrive_link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n","        return gdrive_link\n","    else:\n","        return \"File not found in Google Drive\"\n","\n","def get_shareable_link(url):\n","\n","    file_id = extract_file_id_from_url(url)\n","\n","    auth.authenticate_user()\n","    gauth = GoogleAuth()\n","    gauth.credentials = GoogleCredentials.get_application_default()\n","    drive = GoogleDrive(gauth)\n","\n","    try:\n","        file_obj = drive.CreateFile({'id': file_id})\n","        file_obj.FetchMetadata()\n","        file_obj.InsertPermission({\n","            'type': 'anyone',\n","            'value': 'anyone',\n","            'role': 'reader'\n","        })\n","\n","        # Get the shareable link\n","        return file_obj['alternateLink']\n","    except Exception as e:\n","        print(\"Error:\", e)\n","        return None\n","\n","# if __name__ == \"__main__\":\n","#     # Replace 'YOUR_FILE_ID' with the ID of the file you want to share\n","#     file_id = 'YOUR_FILE_ID'\n","#     shareable_link = get_shareable_link(file_id)\n","#     if shareable_link:\n","#         print(\"Shareable link:\", shareable_link)\n","#     else:\n","#         print(\"Failed to generate shareable link.\")\n"],"metadata":{"id":"6Szf4lrfUwS3","executionInfo":{"status":"ok","timestamp":1713878064828,"user_tz":-60,"elapsed":4,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["# Method Generative Start\n","\n","In this section you will write all details of your Method 1.\n","\n","You will have to enter multiple `code` and `text` cell.\n","\n","Your code should follow the standard ML pipeline\n","\n","\n","*   Data reading\n","*   Data clearning, if any\n","*   Convert data to vector/tokenization/vectorization\n","*   Model Declaration/Initialization/building\n","*   Training and validation of the model using training and validation dataset\n","*   Save the trained model\n","*   Load and Test the model on testing set\n","*   Save the output of the model\n","\n","\n","You could add any other step(s) based on your method's requirement.\n","\n","After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"],"metadata":{"id":"47ywe8jGSKhL"}},{"cell_type":"markdown","source":["#Preprocessing"],"metadata":{"id":"60djetRT2UAp"}},{"cell_type":"code","source":["def preprocessing(file):\n","  df = pd.read_csv(file)\n","  punctuations = string.punctuation\n","  pattern = r\"[^\\w\\s]\"\n","  for i in range(len(df)):\n","    df_train = df.loc[i,'comment'].lower()\n","    df_train = df_train.replace('sdata_2', '')\n","    df_train = df_train.replace('newline_token', '')\n","    df_train = df_train.replace('edata_2', '')\n","    df_train = re.sub(pattern, '', df_train).strip()\n","    df_train = remove_stopwords(df_train)\n","    df_train = word_tokenize(df_train)\n","    df_train = ' '.join(df_train)\n","    df.loc[i,'comment'] = df_train\n","  return df"],"metadata":{"id":"Pmtm53K42V7r","executionInfo":{"status":"ok","timestamp":1713878064828,"user_tz":-60,"elapsed":2,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["file=preprocessing(train_file)\n","file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"NWJM-oZ_2oN_","executionInfo":{"status":"ok","timestamp":1713878073275,"user_tz":-60,"elapsed":8449,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}},"outputId":"0e495e9d-bd47-482b-8fba-eb09bc71d160"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       comment_id                                            comment  split  \\\n","0     578388428.0  apologies time ive intervened wikipedia talk p...  train   \n","1     589196884.0  user report uaa quick conversation let know us...  train   \n","2     651055158.0                               feel resubmit review  train   \n","3     476344881.0  ifa premiership league currently known ifa pre...  train   \n","4      83743858.0  level 1 messagehello thank experimenting wikip...  train   \n","...           ...                                                ...    ...   \n","8694  238667093.0                                      intended page  train   \n","8695  202642785.0                           edits care explain edits  train   \n","8696  254619686.0  second business accept blocked jeff g harassed...  train   \n","8697  550891967.0  pretty clear sockpuppetry excuse sockpuppet ip...  train   \n","8698   79107893.0  simpsonssurely references simpsons earlier ser...  train   \n","\n","      toxicity  \n","0            0  \n","1            0  \n","2            0  \n","3            0  \n","4            1  \n","...        ...  \n","8694         0  \n","8695         0  \n","8696         0  \n","8697         0  \n","8698         0  \n","\n","[8699 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-d90a02d5-8ec0-49b8-a745-2b4b0a4dfdaf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_id</th>\n","      <th>comment</th>\n","      <th>split</th>\n","      <th>toxicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>578388428.0</td>\n","      <td>apologies time ive intervened wikipedia talk p...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>589196884.0</td>\n","      <td>user report uaa quick conversation let know us...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>651055158.0</td>\n","      <td>feel resubmit review</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>476344881.0</td>\n","      <td>ifa premiership league currently known ifa pre...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>83743858.0</td>\n","      <td>level 1 messagehello thank experimenting wikip...</td>\n","      <td>train</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8694</th>\n","      <td>238667093.0</td>\n","      <td>intended page</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8695</th>\n","      <td>202642785.0</td>\n","      <td>edits care explain edits</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8696</th>\n","      <td>254619686.0</td>\n","      <td>second business accept blocked jeff g harassed...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8697</th>\n","      <td>550891967.0</td>\n","      <td>pretty clear sockpuppetry excuse sockpuppet ip...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8698</th>\n","      <td>79107893.0</td>\n","      <td>simpsonssurely references simpsons earlier ser...</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8699 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d90a02d5-8ec0-49b8-a745-2b4b0a4dfdaf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d90a02d5-8ec0-49b8-a745-2b4b0a4dfdaf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d90a02d5-8ec0-49b8-a745-2b4b0a4dfdaf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3a58fd1c-a99b-4e93-925f-04f7dae66c0c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a58fd1c-a99b-4e93-925f-04f7dae66c0c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3a58fd1c-a99b-4e93-925f-04f7dae66c0c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"file","summary":"{\n  \"name\": \"file\",\n  \"rows\": 8699,\n  \"fields\": [\n    {\n      \"column\": \"comment_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 200704603.48277646,\n        \"min\": 4216.0,\n        \"max\": 699826615.0,\n        \"num_unique_values\": 8699,\n        \"samples\": [\n          447988238.0,\n          59651578.0,\n          680890365.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8652,\n        \"samples\": [\n          \"copyrighted documents licensed documents example lot organisations allows material copied ironholds acuse stupidity things think right person admin supervising progress frist sentenced included phrases like banhammer block person reverses template concerned sustaining higher voice background informations copyright template reverted twice\",\n          \"know troll users threats edit pages try like loser sad time edits random mods know come attention thats attention youre troll wikipedia positive cares continue troll abuse power got wiki regardless irrelevant insignificant pathetic attempt think editing privelages 3 days lol\",\n          \"letter b deleted redirect pageit redirect page user page definitely deleted letter b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxicity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","source":["#Machine learning models"],"metadata":{"id":"tojW3tc62Ojg"}},{"cell_type":"markdown","source":["Random forest"],"metadata":{"id":"cUl_YcBw299L"}},{"cell_type":"code","source":["def random_forest():\n","  train_texts = preprocessing(train_file)\n","  val_texts = preprocessing(val_file)\n","\n","\n","  # Define the pipeline\n","  pipeline = Pipeline([\n","      ('tfidf', TfidfVectorizer(max_features=10000)),  # Convert text to TF-IDF features\n","      ('clf', RandomForestClassifier(n_estimators=100, random_state=student_id))  # Random Forest classifier\n","  ])\n","\n","  # Train the model\n","  pipeline.fit(train_texts['comment'], train_texts['toxicity'])\n","\n","  # Predict on validation set\n","  val_predictions = pipeline.predict(val_texts['comment'])\n","\n","  # Calculate accuracy\n","  accuracy = accuracy_score(val_texts['toxicity'], val_predictions)\n","  print(\"Validation Accuracy:\", accuracy)"],"metadata":{"id":"C8MrDBh62R4C","executionInfo":{"status":"ok","timestamp":1713878073275,"user_tz":-60,"elapsed":4,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["SVM"],"metadata":{"id":"tsFrQQhtLrWh"}},{"cell_type":"code","source":["def SVM():\n","  train_df = preprocessing(train_file)\n","  val_df = preprocessing(val_file)\n","\n","  train_texts = train_df\n","  val_texts = val_df\n","\n","  # Define the pipeline\n","\n","\n","  pipeline_svm = Pipeline([\n","      ('vectorizer', TfidfVectorizer()),  # Step 1: TF-IDF vectorization\n","      ('classifier', SVC())  # Step 2: SVM classifier\n","  ])\n","\n","  # Define hyperparameters for GridSearchCV\n","  param_grid_svm = {\n","      # 'classifier__C': [0.1, 0.1, 0.1],  # Regularization parameter\n","      # 'classifier__kernel': ['linear', 'rbf'],  # Kernel type\n","      # 'vectorizer__max_features': [None, 0.1, 0.1]  # Maximum number of features in TF-IDF vectorization\n","  }\n","\n","  # Perform grid search with cross-validation\n","  grid_search_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=5, scoring='accuracy')\n","  grid_search_svm.fit(train_texts['comment'], train_df['toxicity'])\n","\n","  # Get the best model\n","  best_pipeline_svm = grid_search_svm.best_estimator_\n","\n","  # Make predictions\n","  y_pred_svm = best_pipeline_svm.predict(val_texts['comment'])\n","\n","  # Calculate accuracy\n","  accuracy_svm = accuracy_score(val_df['toxicity'], y_pred_svm)\n","  print(\"Accuracy (SVM):\", accuracy_svm)"],"metadata":{"id":"5iayQRikKdCF","executionInfo":{"status":"ok","timestamp":1713878073275,"user_tz":-60,"elapsed":3,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["Naive Bayes"],"metadata":{"id":"t4Hiv-QmGjLr"}},{"cell_type":"code","source":["def naive_bayes(train_file, val_file):\n","    train_texts = preprocessing(train_file)\n","    val_texts = preprocessing(val_file)\n","\n","    # Define the pipeline\n","    pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer(max_features=10000)),  # Convert text to TF-IDF features\n","        ('clf', MultinomialNB())  # Naive Bayes classifier\n","    ])\n","\n","    # Train the model\n","    pipeline.fit(train_texts['comment'], train_texts['toxicity'])\n","\n","    # Predict on validation set\n","    val_predictions = pipeline.predict(val_texts['comment'])\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(val_texts['toxicity'], val_predictions)\n","    print(\"Validation Accuracy (Naive Bayes):\", accuracy)"],"metadata":{"id":"teVxcndhGmQc","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":481,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["Logistic regression"],"metadata":{"id":"cSFNkvP0Wr6R"}},{"cell_type":"code","source":["def logistic_regression(train_file, val_file):\n","    train_texts = preprocessing(train_file)\n","    val_texts = preprocessing(val_file)\n","\n","    # Define the pipeline\n","    pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer(max_features=10000)),  # Convert text to TF-IDF features\n","        ('clf', LogisticRegression())  # Logistic Regression classifier\n","    ])\n","\n","    # Train the model\n","    pipeline.fit(train_texts['comment'], train_texts['toxicity'])\n","\n","    # Predict on validation set\n","    val_predictions = pipeline.predict(val_texts['comment'])\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(val_texts['toxicity'], val_predictions)\n","    print(\"Validation Accuracy (Logistic Regression):\", accuracy)"],"metadata":{"id":"_5441u7TWtpj","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":90,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["Neural network model"],"metadata":{"id":"dHurk5vQNKwG"}},{"cell_type":"markdown","source":["LSTM"],"metadata":{"id":"_NLtAcIdNNge"}},{"cell_type":"code","source":["def lstm():\n","  class CustomDataset(Dataset):\n","      def __init__(self, comments, toxicities, tokenizer, vocab, max_seq_length):\n","          self.comments = comments\n","          self.toxicities = toxicities\n","          self.tokenizer = tokenizer\n","          self.vocab = vocab\n","          self.max_seq_length = max_seq_length\n","\n","      def __len__(self):\n","          return len(self.comments)\n","\n","      def __getitem__(self, idx):\n","          comment = self.comments[idx]\n","          toxicity = self.toxicities[idx]\n","\n","          # Tokenize and pad the comment\n","          tokens = self.tokenizer(comment)[:self.max_seq_length]\n","          indexed_tokens = [self.vocab[token] for token in tokens]\n","          padded_tokens = indexed_tokens + [0] * (self.max_seq_length - len(indexed_tokens))\n","\n","          return {\n","              'input_ids': torch.tensor(padded_tokens, dtype=torch.long),\n","              'toxicity': torch.tensor(toxicity, dtype=torch.float32)\n","          }\n","\n","  # Define hyperparameters\n","  embedding_dim = 100  # Dimension of word embeddings\n","  hidden_dim = 128  # Dimension of LSTM hidden state\n","  learning_rate = 0.001\n","  num_epochs = 5\n","  max_seq_length = 50  # Maximum sequence length for padding\n","\n","  # Tokenize comments (replace this with your tokenizer)\n","  def tokenize(comment):\n","      return comment.split()\n","  train_df = preprocessing(train_file)\n","  val_df = preprocessing(val_file)\n","  # Load and preprocess the data\n","  train_comments = train_df['comment'].tolist()\n","  train_toxicities = train_df['toxicity'].tolist()\n","  val_comments = val_df['comment'].tolist()\n","  val_toxicities = val_df['toxicity'].tolist()\n","\n","  # Define vocabulary (replace this with your actual vocabulary)\n","  vocab = set(tokenize(' '.join(train_comments + val_comments)))\n","  vocab_size = len(vocab)\n","  vocab = {token: idx + 1 for idx, token in enumerate(vocab)}  # Assign unique index to each token, starting from 1\n","\n","  # Create data loaders\n","  train_dataset = CustomDataset(train_comments, train_toxicities, tokenize, vocab, max_seq_length)\n","  val_dataset = CustomDataset(val_comments, val_toxicities, tokenize, vocab, max_seq_length)\n","  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=64)\n","\n","  # Define the LSTM model\n","  class LSTMModel(nn.Module):\n","      def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","          super(LSTMModel, self).__init__()\n","          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","          self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","          self.fc = nn.Linear(hidden_dim, 1)\n","\n","      def forward(self, x):\n","          embedded = self.embedding(x)\n","          lstm_out, _ = self.lstm(embedded)\n","          output = self.fc(lstm_out[:, -1, :])\n","          return output.squeeze(1)\n","\n","  # Instantiate the model\n","  model = LSTMModel(vocab_size + 1, embedding_dim, hidden_dim)\n","\n","  # Define loss function and optimizer\n","  criterion = nn.BCEWithLogitsLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Training loop\n","  for epoch in range(num_epochs):\n","      model.train()\n","      for batch in train_loader:\n","          inputs = batch['input_ids']\n","          labels = batch['toxicity']\n","          optimizer.zero_grad()\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)  # Remove unsqueeze(1) here\n","          loss.backward()\n","          optimizer.step()\n","      print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n","\n","  # Evaluation\n","  model.eval()\n","  correct = 0\n","  total = 0\n","\n","  predicted_labels = []\n","  with torch.no_grad():\n","      for batch in val_loader:\n","          inputs = batch['input_ids']\n","          labels = batch['toxicity']\n","          outputs = model(inputs)\n","          predicted = torch.round(torch.sigmoid(outputs))  # Predicted values (0 or 1)\n","          predicted_labels.extend(predicted.tolist())\n","          batch_size = labels.size(0)\n","          total += batch_size\n","          correct += (predicted == labels[:batch_size]).sum().item()  # Adjust for the size of the last batch\n","\n","  accuracy = correct / total\n","  print(f'Accuracy on validation set: {accuracy}')"],"metadata":{"id":"TgORO4lFNNJr","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":90,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["CNN"],"metadata":{"id":"2odYZY4TQBBv"}},{"cell_type":"code","source":["def cnn():\n","  class CustomDataset(Dataset):\n","      def __init__(self, comments, toxicities, tokenizer, vocab, max_seq_length):\n","          self.comments = comments\n","          self.toxicities = toxicities\n","          self.tokenizer = tokenizer\n","          self.vocab = vocab\n","          self.max_seq_length = max_seq_length\n","\n","      def __len__(self):\n","          return len(self.comments)\n","\n","      def __getitem__(self, idx):\n","          comment = self.comments[idx]\n","          toxicity = self.toxicities[idx]\n","\n","          # Tokenize and pad the comment\n","          tokens = self.tokenizer(comment)[:self.max_seq_length]\n","          indexed_tokens = [self.vocab[token] for token in tokens]\n","          padded_tokens = indexed_tokens + [0] * (self.max_seq_length - len(indexed_tokens))\n","\n","          return {\n","              'input_ids': torch.tensor(padded_tokens, dtype=torch.long),\n","              'toxicity': torch.tensor(toxicity, dtype=torch.float32)\n","          }\n","\n","  # Define hyperparameters\n","  embedding_dim = 100  # Dimension of word embeddings\n","  num_filters = 128  # Number of filters in the convolutional layer\n","  filter_sizes = [2, 3, 4]  # Kernel sizes for the convolutional layer\n","  learning_rate = 0.001\n","  num_epochs = 5\n","  max_seq_length = 50  # Maximum sequence length for padding\n","\n","  # Tokenize comments (replace this with your tokenizer)\n","  def tokenize(comment):\n","      return comment.split()\n","\n","  # Load and preprocess the data\n","  train_df = preprocessing(train_file)\n","  val_df = preprocessing(val_file)\n","  train_comments = train_df['comment'].tolist()\n","  train_toxicities = train_df['toxicity'].tolist()\n","  val_comments = val_df['comment'].tolist()\n","  val_toxicities = val_df['toxicity'].tolist()\n","\n","  # Define vocabulary (replace this with your actual vocabulary)\n","  vocab = set(tokenize(' '.join(train_comments + val_comments)))\n","  vocab_size = len(vocab)\n","  vocab = {token: idx + 1 for idx, token in enumerate(vocab)}  # Assign unique index to each token, starting from 1\n","\n","  # Create data loaders\n","  train_dataset = CustomDataset(train_comments, train_toxicities, tokenize, vocab, max_seq_length)\n","  val_dataset = CustomDataset(val_comments, val_toxicities, tokenize, vocab, max_seq_length)\n","  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=64)\n","\n","  # Define the CNN model\n","  class CNNModel(nn.Module):\n","      def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes):\n","          super(CNNModel, self).__init__()\n","          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","          self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (filter_size, embedding_dim)) for filter_size in filter_sizes])\n","          self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n","\n","      def forward(self, x):\n","          embedded = self.embedding(x).unsqueeze(1)  # Add channel dimension\n","          conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # Apply convolution and ReLU activation\n","          pooled = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conved]  # Apply max pooling\n","          cat = torch.cat(pooled, dim=1)  # Concatenate the pooled features\n","          output = self.fc(cat)\n","          return output.squeeze(1)\n","\n","  # Instantiate the model\n","  model = CNNModel(vocab_size + 1, embedding_dim, num_filters, filter_sizes)\n","\n","  # Define loss function and optimizer\n","  criterion = nn.BCEWithLogitsLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # Training loop\n","  for epoch in range(num_epochs):\n","      model.train()\n","      for batch in train_loader:\n","          inputs = batch['input_ids']\n","          labels = batch['toxicity']\n","          optimizer.zero_grad()\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","      print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n","\n","  # Evaluation\n","  model.eval()\n","  correct = 0\n","  total = 0\n","\n","  predicted_labels = []\n","  with torch.no_grad():\n","      for batch in val_loader:\n","          inputs = batch['input_ids']\n","          labels = batch['toxicity']\n","          outputs = model(inputs)\n","          predicted = torch.round(torch.sigmoid(outputs))  # Predicted values (0 or 1)\n","          predicted_labels.extend(predicted.tolist())\n","          batch_size = labels.size(0)\n","          total += batch_size\n","          correct += (predicted == labels[:batch_size]).sum().item()  # Adjust for the size of the last batch\n","\n","  accuracy = correct / total\n","  print(f'Accuracy on validation set: {accuracy}')"],"metadata":{"id":"1LSa05EAQB6o","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":89,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["## Training Generative Method Code\n","Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n","\n","Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."],"metadata":{"id":"1sA3OWlVbnoY"}},{"cell_type":"markdown","source":["For generative using Naive Bayes because in generative models it gave the highest accuracy"],"metadata":{"id":"xplEYbp-dsis"}},{"cell_type":"code","source":["def train_Gen(train_file, val_file, model_dir):\n","    \"\"\"\n","    Takes train_file, val_file and model_dir as input.\n","    It trained on the train_file datapoints, and validate on the val_file datapoints.\n","    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n","    After finishing the training, it saved the best model in the model_dir.\n","\n","    ADD Other arguments, if needed.\n","\n","    Args:\n","        train_file: Train file name\n","        val_file: Validation file name\n","        model_dir: Model output Directory\n","\n","\n","\n","    \"\"\"\n","\n","    ##########################################################################\n","    #                     TODO: Implement this function                      #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","    train_texts = preprocessing(train_file)\n","    val_texts = preprocessing(val_file)\n","\n","    # Define the pipeline\n","    pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer(max_features=10000)),  # Convert text to TF-IDF features\n","        ('clf', MultinomialNB())  # Naive Bayes\n","    ])\n","\n","    # Train the model\n","    pipeline.fit(train_texts['comment'], train_texts['toxicity'])\n","\n","    # Predict on validation set\n","    val_predictions = pipeline.predict(val_texts['comment'])\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(val_texts['toxicity'], val_predictions)\n","    print(\"Validation Accuracy (Naive Bayes):\", accuracy)\n","    save_model(pipeline,model_dir)\n","\n","    # Now Zip Model to share it\n","    zip_directory(model_dir, MODEL_Gen_File)\n","\n","    model_gdrive_link = get_gdrive_link(MODEL_Gen_File)\n","\n","    print(model_gdrive_link)\n","    get_shareable_link(model_gdrive_link)\n","\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################\n","    return model_gdrive_link"],"metadata":{"id":"_Ux7GBKXbqq6","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":89,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["## Testing Method 1 Code\n","Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "],"metadata":{"id":"qyJ_xv12Uy9c"}},{"cell_type":"code","source":["def test_Gen(test_file, MODEL_PATH,model_gdrive_link):\n","    \"\"\"\n","    take test_file, model_file and output_dir as input.\n","    It loads model and test of the examples in the test_file.\n","    It prints different evaluation metrics, and saves the output in output directory\n","\n","    ADD Other arguments, if needed\n","\n","    Args:\n","        test_file: test file name\n","        model_gdrive_link: GDrive URL\n","        MODEL_PATH: Directory of Model\n","\n","    \"\"\"\n","\n","    print('\\n Start by downloading model')\n","    # # Frist Get model from the link\n","    # # model_gdrive_link = get_gdrive_link(model_file)\n","\n","    # # These two are temporary directory and file\n","    test_model_file = MODEL_PATH+'/test.zip'\n","    test_model_path = MODEL_PATH+'/test/'\n","\n","    # # Now download and unzip the model file\n","    download_zip_file_from_link(model_gdrive_link,test_model_file)\n","    unzip_file(test_model_file, test_model_path)\n","    print('\\n Model is downloaded to ',test_model_path)\n","    ##########################################################################\n","    #                     TODO: Implement this function                      #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    # Let's get test data\n","    test_df = pd.read_csv(test_file)\n","    print('\\n Data is loaded from ', test_file)\n","\n","    # Let's get the model file name & load it\n","    # Note you have to use same name a you used in the save\n","\n","    test_model_file = os.path.join(test_model_path, 'Model_Gen', 'model.sav')\n","\n","    model = load_model(test_model_file)\n","\n","    # Let's do the prediction using test data\n","    y_pred= model.predict(test_df['comment'])\n","\n","    # Now save the model output in the same test file\n","    # Note the name of output column, this is for the discriminative model\n","    test_df['out_label_model_Gen'] = y_pred\n","\n","    # Now save the model output in the same output file\n","    test_df.to_csv(test_file, index=False)\n","    print('\\n Output is save in ', test_file)\n","\n","    #########################################################################\n","                              #  END OF YOUR CODE                            #\n","    #########################################################################\n","    return test_file"],"metadata":{"id":"43T3JqK5a484","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":89,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["## Method Generative End\n"],"metadata":{"id":"ue3xIDFGSXNH"}},{"cell_type":"markdown","source":["# Method Discriminative Start\n","\n","In this section you will write all details of your Method 2.\n","\n","You will have to enter multiple `code` and `text` cell.\n","\n","Your code should follow the standard ML pipeline\n","\n","\n","*   Data reading\n","*   Data clearning, if any\n","*   Convert data to vector/tokenization/vectorization\n","*   Model Declaration/Initialization/building\n","*   Training and validation of the model using training and validation dataset\n","*   Save the trained model\n","*   Load and Test the model on testing set\n","*   Save the output of the model\n","\n","You could add any other step(s) based on your method's requirement.\n","\n","After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"],"metadata":{"id":"o5jNIHneSfzg"}},{"cell_type":"markdown","source":["## Training Method Discriminative Code\n","Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n","\n","Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."],"metadata":{"id":"zAkC0CWAc1BY"}},{"cell_type":"markdown","source":["For discriminative I used Random forest beacues it gave highest accuracy for discriminative model"],"metadata":{"id":"WtiTdqVBd261"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline"],"metadata":{"id":"J892iPM8hFvJ","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":88,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["def train_dis(train_file, val_file, model_dir):\n","    \"\"\"\n","    Takes train_file, val_file and model_dir as input.\n","    It trained on the train_file datapoints, and validate on the val_file datapoints.\n","    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n","    After finishing the training, it saved the best model in the model_dir.\n","\n","    ADD Other arguments, if needed.\n","\n","    Args:\n","        train_file: Train file name\n","        val_file: Validation file name\n","        model_dir: Model output Directory\n","\n","    \"\"\"\n","\n","    ##########################################################################\n","    #                     TODO: Implement this function                      #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    train_texts = preprocessing(train_file)\n","    val_texts = preprocessing(val_file)\n","\n","\n","    # Define the pipeline\n","    pipeline = Pipeline([\n","        ('tfidf', TfidfVectorizer(max_features=10000)),  # Convert text to TF-IDF features\n","        ('clf', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest classifier\n","    ])\n","\n","    # Train the model\n","    pipeline.fit(train_texts['comment'], train_texts['toxicity'])\n","\n","    # Predict on validation set\n","    val_predictions = pipeline.predict(val_texts['comment'])\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(val_texts['toxicity'], val_predictions)\n","    print(\"Validation Accuracy:\", accuracy)\n","\n","    # Model is working fine, so save model\n","    # Note modify this with your code\n","    save_model(pipeline,model_dir)\n","\n","    # Now Zip Model to share it\n","    zip_directory(model_dir, MODEL_Dis_File)\n","\n","    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)\n","\n","    print(model_gdrive_link)\n","    get_shareable_link(model_gdrive_link)\n","\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################\n","    return model_gdrive_link"],"metadata":{"id":"JA7vbLnoc0b5","executionInfo":{"status":"ok","timestamp":1713878073754,"user_tz":-60,"elapsed":88,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["## Testing Method Discriminative Code\n","Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "],"metadata":{"id":"LVi2vGeZc5Fp"}},{"cell_type":"code","source":["def test_dis(test_file, MODEL_PATH,model_gdrive_link):\n","    \"\"\"\n","    take test_file, model_file and output_dir as input.\n","    It loads model and test of the examples in the test_file.\n","    It prints different evaluation metrics, and saves the output in output directory\n","\n","    ADD Other arguments, if needed\n","\n","    Args:\n","        test_file: test file name\n","        model_gdrive_link: GDrive URL\n","        MODEL_PATH: Directory of Model\n","\n","    \"\"\"\n","    print('\\n Start by downloading model')\n","    # Frist Get model from the link\n","    # model_gdrive_link = get_gdrive_link(model_file)\n","\n","    # These two are temporary directory and file\n","    test_model_file = MODEL_PATH+'/test.zip'\n","    test_model_path = MODEL_PATH+'/test/'\n","\n","    # Now download and unzip the model file\n","    download_zip_file_from_link(model_gdrive_link,test_model_file)\n","    unzip_file(test_model_file, test_model_path)\n","    print('\\n Model is downloaded to ',test_model_path)\n","    ##########################################################################\n","    #                     TODO: Implement this function                      #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    # Let's get test data\n","    test_df = pd.read_csv(test_file)\n","    print('\\n Data is loaded from ', test_file)\n","\n","    # Let's get the model file name & load it\n","    # Note you have to use same name a you used in the save\n","\n","    test_model_file = os.path.join(test_model_path, 'Model_Dis', 'model.sav')\n","\n","    model = load_model(test_model_file)\n","\n","    # Let's do the prediction using test data\n","    y_pred= model.predict(test_df['comment'])\n","\n","    # Now save the model output in the same test file\n","    # Note the name of output column, this is for the discriminative model\n","    test_df['out_label_model_Dis'] = y_pred\n","\n","    # Now save the model output in the same output file\n","    test_df.to_csv(test_file, index=False)\n","    print('\\n Output is save in ', test_file)\n","\n","    ##########################################################################\n","    #                            END OF YOUR CODE                            #\n","    ##########################################################################\n","    return test_file"],"metadata":{"id":"v477tmxOSlVj","executionInfo":{"status":"ok","timestamp":1713878073755,"user_tz":-60,"elapsed":89,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["## Discriminative Method  End\n"],"metadata":{"id":"4gNwyxXNSmVb"}},{"cell_type":"markdown","source":[],"metadata":{"id":"bDYDtWqMltOq"}},{"cell_type":"markdown","source":["# Other Method/model Start"],"metadata":{"id":"rmaJfJkVwSDW"}},{"cell_type":"code","source":["import argparse"],"metadata":{"id":"YsIl8SPc6uen","executionInfo":{"status":"ok","timestamp":1713878074013,"user_tz":-60,"elapsed":262,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# Define argparse-like function\n","def parse_arguments(option):\n","    parser = argparse.ArgumentParser(description='Process some integers.')\n","    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')\n","    args = parser.parse_args(args=[])\n","    return args\n","\n","# Function to perform some action based on selected option\n","def perform_action(option):\n","    print(\"Performing action with option:\", option)\n","\n","    if option == '0':\n","      print('\\n Okay Exiting!!! ')\n","\n","    elif option == '1':\n","      print('\\n Training Generative Model')\n","      model_gdrive_link = train_Gen(train_file,val_file,MODEL_Gen_DIRECTORY)\n","      print('Make sure to pass model URL in Testing',model_gdrive_link)\n","\n","    elif option == '2':\n","      print('\\n\\n Pass the URL Not Variable !!!')\n","      print('\\n Testing Generative Model')\n","      model_gen_url = 'https://drive.google.com/file/d/1-515xZUkAD1589McBX6wWuy67axjoGgl/view?usp=sharing'\n","      test_Gen(test_file,MODEL_PATH, model_gen_url)\n","\n","    elif option == '3':\n","      print('\\n Training Disciminative Model')\n","      model_gdrive_link = train_dis(train_file,val_file,MODEL_Dis_DIRECTORY)\n","      print('Make sure to pass model URL in Testing',model_gdrive_link)\n","      print('\\n\\n Pass the URL Not Variable !!!')\n","\n","    elif option == '4':\n","      print('\\n\\n Pass the URL Not Variable !!!')\n","      print('\\n Testing Disciminative Model')\n","      model_dis_url = 'https://drive.google.com/file/d/1--sJNJMLFvAz77HjiB8is8DDHOfY0FBG/view?usp=sharing'\n","      test_dis(test_file, MODEL_PATH, model_dis_url)\n","      # test_dis(test_file, MODEL_PATH, model_gdrive_link)\n","\n","    elif option == '5':\n","      print(\"SVM model\")\n","      SVM()\n","\n","    elif option == '6':\n","      print(\"logistic regression model\")\n","      logistic_regression(train_file, val_file)\n","\n","    elif option == '7':\n","      print(\"LSTM model\")\n","      lstm()\n","\n","    elif option == '8':\n","      print(\"CNN model\")\n","      cnn()\n","\n","    else:\n","      print('Wrong Option Selected. \\n\\nPlease select Correct option')\n","      main()\n","\n","\n","def main():\n","\n","    # Get option from user input\n","    user_option = input(\"0. To Exit Code\\n\"\n","                     \"1. Train Model Generative\\n\"\n","                    \"2. Test Model Generative\\n\"\n","                    \"3. Train Model Discriminative\\n\"\n","                    \"4. Test Model Discriminative\\n\"\n","                    \"5. SVM model\\n\"\n","                    \"6. Logistic regression\\n\"\n","                    \"7. LSTM model\\n\"\n","                    \"8. CNN model\\n\"\n","                    \"Enter your option: \")\n","\n","    args = parse_arguments(user_option)\n","    option = args.option\n","    perform_action(option)\n"],"metadata":{"id":"vWBxKhLI6vfY","executionInfo":{"status":"ok","timestamp":1713878074013,"user_tz":-60,"elapsed":2,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"LQTmVWa5wdD3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe5fbaa6-8850-476d-a9f7-1787dfff218b","executionInfo":{"status":"ok","timestamp":1713878077798,"user_tz":-60,"elapsed":3787,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["0. To Exit Code\n","1. Train Model Generative\n","2. Test Model Generative\n","3. Train Model Discriminative\n","4. Test Model Discriminative\n","5. SVM model\n","6. Logistic regression\n","7. LSTM model\n","8. CNN model\n","Enter your option: 0\n","Performing action with option: 0\n","\n"," Okay Exiting!!! \n"]}]},{"cell_type":"code","source":["model_dis_url = 'https://drive.google.com/file/d/1--sJNJMLFvAz77HjiB8is8DDHOfY0FBG/view?usp=sharing'\n","test_dis(test_file, MODEL_PATH, model_dis_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"21nlQsMVRTJe","outputId":"3c1bab49-c2c0-473a-b036-848b63f39b0d","executionInfo":{"status":"ok","timestamp":1713878080533,"user_tz":-60,"elapsed":2738,"user":{"displayName":"Vishnu Vk","userId":"00208390028597509822"}}},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Start by downloading model\n","File downloaded successfully! /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/test.zip\n","Extracted a zip file to /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/test/\n","\n"," Model is downloaded to  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/test/\n","\n"," Data is loaded from  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/test.csv\n","Loaded model from  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/model/2310092/test/Model_Dis/model.sav\n","\n"," Output is save in  /content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/test.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/MyDrive/university of essex/text analysis/text analysis assignment final final/data/2/test.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["##Other Method/model End"],"metadata":{"id":"7yMswIeAwYIf"}}]}